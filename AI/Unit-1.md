UNIT I INTELLIGENT AGENTS


1. Introduction to AI
Explanation Artificial Intelligence (AI) is a branch of computer science focused on creating intelligent machines that can behave like humans, think like humans, and make decisions. AI exists when a machine possesses human-based skills such as learning, reasoning, and solving problems. It is a widely growing field of computer science that holds the potential to cause a machine to work like a human. AI stands for "a man-made thinking power," as Artificial defines "man-made" and intelligence defines "thinking power". AI systems use programmed algorithms and can work with their own intelligence, meaning you do not need to preprogram them to do specific work. It is considered a booming technology, currently working in subfields such as self-driving cars, playing chess, proving theorems, and painting.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists (No specific diagram for the introduction is provided.)
Output or Result Possible applications include: solving real-world problems easily and with accuracy (like health or traffic issues), creating personal virtual assistants (Siri, Google Assistant), building robots for risky environments, and opening paths for new technologies,.
--------------------------------------------------------------------------------
1.1 Why Artificial Intelligence?
Explanation AI is important because it allows the creation of software and devices that can solve real-world problems (e.g., health, traffic, marketing) easily and with accuracy. It facilitates the building of personal virtual assistants, such as Cortana, Siri, and Google Assistant. Furthermore, AI can be used to construct robots capable of working in environments where human survival might be at risk. AI also fosters new technologies, opportunities, and devices.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists (No specific diagram is provided.)
Output or Result AI helps in solving real-world problems easily and accurately.
--------------------------------------------------------------------------------
1.2 Goals of Artificial Intelligence
Explanation The main goals of AI include replicating human intelligence, solving knowledge-intensive tasks, and creating an intelligent connection between perception and action. AI aims to build machines that can perform tasks requiring human intelligence, such as proving theorems, playing chess, planning surgical operations, or driving a car in traffic,. Ultimately, a goal is creating a system that exhibits intelligent behaviour, learns new things independently, demonstrates, explains, and advises its user.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists (No specific diagram is provided.)
Output or Result The primary output is a machine that performs tasks requiring human intelligence, exhibiting intelligent behaviour, and making decisions.
--------------------------------------------------------------------------------
1.3 What Comprises to Artificial Intelligence?
Explanation AI is a vast field that requires contributions from many disciplines; it is not solely a part of computer science. Intelligence itself is an intangible combination of Reasoning, learning, problem-solving, perception, and language understanding. To achieve these factors in a machine, AI requires disciplines such as Mathematics, Biology, Psychology, Sociology, Computer Science, Statistics, Philosophy, and Neuron Study.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists The relationship between AI and contributing disciplines is represented in a diagram:
ARTIFICIAL INTELLIGENCE
Maths, Neuron Science, Philosophy, Biology, Computer Science, Sociology, Psychology, Statistics
Output or Result AI systems are built by combining concepts from various contributing fields to mimic human intelligence,.
--------------------------------------------------------------------------------
1.4 Advantages of Artificial Intelligence
Explanation AI systems offer several benefits, including High Accuracy with less error, as decisions are based on pre-experience or information. They offer High-Speed decision-making, allowing them to beat chess champions. AI machines have High reliability, capable of performing the same action repeatedly with high accuracy. They are Useful for risky areas where deploying humans is unsafe, such as defusing bombs or exploring the ocean floor. AI provides Digital Assistants (used by e-commerce sites for product recommendations) and serves as a public utility (like facial recognition for security or self-driving cars),.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists (No specific diagram is provided.)
Output or Result Results include highly accurate and reliable performance, fast decision-making, and enhanced safety/utility in dangerous or common applications,,.
--------------------------------------------------------------------------------
1.5 Disadvantages of Artificial Intelligence
Explanation Disadvantages of AI include the High Cost required for the necessary hardware, software, and maintenance. AI machines Can't think out of the box; they only perform the work for which they were trained or programmed. They lack feelings and emotions, meaning they cannot form emotional attachments and may pose a risk if proper care is not taken. Increased use of AI leads to an Increase in dependency on machines, potentially resulting in the loss of human mental capabilities. Finally, AI machines possess No Original Creativity or imaginative power, which humans excel at.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists (No specific diagram is provided.)
Output or Result Constraints on AI include high expense and a lack of adaptability, emotion, and original creativity,.
--------------------------------------------------------------------------------
1.6 History of Artificial Intelligence
Explanation AI is an old concept, dating back to myths of mechanical men in Ancient Greek and Egyptian times. The history includes milestones defining its development.
• Maturation (1943-1952): In 1943, Warren McCulloch and Walter Pits proposed the first model of artificial neurons. In 1950, Alan Turing published "Computing Machinery and Intelligence," proposing the Turing test to check if a machine exhibits intelligence equivalent to humans.
• Birth of AI (1952-1956): In 1955, Allen Newell and Herbert A. Simon created the "Logic Theorist," the first AI program. In 1956, John McCarthy coined the term "Artificial Intelligence" at the Dartmouth Conference, establishing AI as an academic field.
• Golden Years (1956-1974): Joseph Weizenbaum created the first chatbot, ELIZA, in 1966. WABOT-1, the first intelligent humanoid robot, was built in Japan in 1972.
• First AI Winter (1974-1980): This period saw a severe shortage of government funding for AI research, and public interest decreased,.
• A Boom of AI (1980-1987): AI returned with "Expert Systems," which emulated human expert decision-making ability.
• Second AI Winter (1987-1993): Funding stopped again due to high costs and inefficient results from systems like XCON.
• Emergence of Intelligent Agents (1993-2011): In 1997, IBM Deep Blue became the first computer to beat a world chess champion (Gary Kasparov). By 2002, AI entered the home via Roomba vacuum cleaners.
• Deep learning, big data and artificial general intelligence (2011- present): IBM's Watson won the quiz show Jeopardy in 2011, proving its ability to understand natural language. In 2014, the chatbot "Eugene Goostman" won the Turing test.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A historical timeline shows major milestones:
Year
Event
1943
Evolution of Artificial neurons
1950
Turing Machine
1956
Birth of AI: Dartmouth Conference
1966
First Chatboat: ELIZA
1972
First Intelligent Robot: WABOT-1
1980
Expert System
1997
IBM Deep blue: first computer to beat a world chess champion
2002
AI in Home: Roomba
2011
IBM’s Watson: Wins a quiz show
2014
Chatbot Eugene Goostman: Wins "Turing test"
Output or Result The result of this history is that AI has developed to a remarkable level, with current trends involving Deep learning, big data, and data science, inspiring a future with high intelligence.
--------------------------------------------------------------------------------
1.7 Types of Artificial Intelligence
Explanation AI is primarily categorized in two ways: based on capabilities (Type 1) and based on functionality (Type 2).
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists The flow diagram illustrates the types:
Artificial Intelligence
Type - 1 (Based on Capabilities): Narrow AI, General AI, Strong AI
Type - 2 (Based on Functionality): Reactive Machines, Limited Memory, Theory of Mind, Self-Awareness
Output or Result A categorization framework for analyzing the degree of intelligence or capability of an AI system.
--------------------------------------------------------------------------------
1.7.1 Type - 1: Based on Capabilities (Narrow, General, Super AI)
Explanation
1. Weak AI or Narrow AI: This is the most common and currently available AI. It performs a single dedicated task intelligently and cannot operate beyond its trained field or limitations. Examples include Apple Siri, IBM’s Watson, self-driving cars, and purchasing suggestions on e-commerce sites.
2. General AI: This refers to intelligence that could perform any intellectual task with human-like efficiency. The goal is to create systems that can think like a human independently. Currently, no such system exists.
3. Super AI (Strong AI): This is a hypothetical concept where machines surpass human intelligence, performing any task better than humans using cognitive properties,. Key characteristics include the ability to think, reason, solve puzzles, make judgments, plan, learn, and communicate on their own.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A table summarizing the capabilities:
Type
Purpose
Status
Narrow AI
Dedicated for one task
We are here
General AI
Perform like human
Under research
Super AI
Intelligent than human
Hypothetical
Output or Result A progression of AI capabilities, from Narrow AI (task-specific intelligence) to hypothetical Super AI (superior cognitive functions to humans),.
--------------------------------------------------------------------------------
1.7.2 Type - 2: Based on Functionality (Reactive, Limited Memory, Theory of Mind, Self-Awareness)
Explanation
1. Reactive Machines: These are the most basic types of AI. They focus only on current scenarios and react optimally, without storing memories or past experiences for future actions. IBM's Deep Blue and Google's AlphaGo are examples.
2. Limited Memory: These machines can store past experiences or data for a short period. They use this stored data for limited time periods; for example, self-driving cars store recent speeds of nearby cars, distance, and speed limits to navigate.
3. Theory of Mind: This hypothetical AI type is designed to understand human emotions, beliefs, and intentions, enabling them to interact socially like humans.
4. Self-Awareness: This is the future, hypothetical stage of AI where machines possess their own consciousness, sentiments, and self-awareness, making them smarter than the human mind,.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists (No specific diagram is provided, but the flow diagram in 1.7 shows these types under Type - 2).
Output or Result A framework showing AI systems based on their ability to use past data or simulate human cognitive understanding and consciousness,.
--------------------------------------------------------------------------------
1.8 Application of AI
Explanation AI is essential today because it efficiently solves complex problems across various industries.
• AI in Healthcare: Used for faster and better diagnoses and informing doctors when patients are worsening,.
• AI in Gaming: Used for strategic games like chess, where the machine must consider many possible moves.
• AI in Finance: Implements automation, chatbots, algorithm trading, and machine learning into financial processes.
• AI in Data Security: Used to secure data and determine software bugs and cyber-attacks.
• AI in Social Media: Helps organize and manage massive amounts of user data, identifying trends and user requirements.
• AI in Travel & Transport: Used for travel arrangements, suggesting hotels and routes, and powering chatbots for fast customer response.
• AI in Automotive Industry: Used for self-driving cars to enhance safety and provide virtual assistants like TeslaBot,.
• AI in Robotics: Creates intelligent robots that can perform tasks based on their own experiences without pre-programming, such as humanoid robots like Sophia and Erica.
• AI in Entertainment: ML/AI algorithms are used by services like Netflix and Amazon to provide program recommendations.
• AI in Agriculture: Used through agriculture robotics, soil/crop monitoring, and predictive analysis to assist farmers.
• AI in E-commerce: Provides a competitive edge by helping shoppers find associated products, suggesting size, colour, or brand.
• AI in education: Can automate grading and function as teaching assistants or personal virtual tutors for students.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A list of sectors where AI is applied: Astronomy, Data Security, Social Media, Automotive, Robotics, Finance, Entertainment, Gaming, E-commerce, Healthcare, Transport, Agriculture, Education.
Output or Result AI makes daily life faster and more comfortable by solving complex problems efficiently across nearly all major sectors.
--------------------------------------------------------------------------------
2. Agents and Environments
Explanation An AI system is fundamentally the study of a rational agent and its environment. An Agent is anything that perceives its environment through sensors and acts upon that environment through actuators. The agent operates in a cycle of perceiving, thinking, and acting.
• Human-Agent: Uses eyes and ears as sensors, and hands/legs as actuators.
• Robotic Agent: Uses cameras and NLP for sensors, and motors for actuators.
• Software Agent: Uses keystrokes and file contents as sensory input, and display output on the screen.
Supporting Definitions:
• Sensor: A device that detects changes in the environment and sends this information to electronic devices; agents observe the environment via sensors.
• Actuators: Components that convert energy into motion, responsible for moving and controlling a system (e.g., electric motors, gears).
• Effectors: Devices that physically affect the environment (e.g., legs, arms, display screens).
• Environment: Everything surrounding the agent that is not part of the agent itself; it provides the agent with something to sense and act upon.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A conceptual diagram illustrating agent interaction:
Percepts
Environment
Actions
Sensors →
Agent
→ Effectors
Output or Result The agent processes percepts from the environment and executes actions back into the environment via effectors/actuators,.
--------------------------------------------------------------------------------
2.2 Intelligent Agents
Explanation An intelligent agent is an autonomous entity that acts upon an environment using sensors and actuators to achieve defined goals. Intelligent agents have the capability to learn from the environment to meet their goals. A thermostat is an example of an intelligent agent.
Four Main Rules for an AI Agent:
1. Must have the ability to perceive the environment.
2. The observation (percepts) must be used to make decisions.
3. The decision should lead to an action.
4. The action taken must be a rational action.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists (No specific diagram is provided, but the diagram in 2.1 applies here.)
Output or Result The result is an agent that takes rational actions based on its observations to achieve its predetermined goals,.
--------------------------------------------------------------------------------
2.3 Types of Al Agents
Explanation Agents are grouped into five classes based on their perceived intelligence and capability; all can improve their performance over time.
2.3.1 Simple Reflex Agent
Explanation These are the simplest agents, making decisions solely based on current percepts and ignoring past percept history. They only operate successfully in a fully observable environment. They function using Condition-action rules, mapping the current state directly to an action (e.g., a room cleaner only works if there is dirt).
Better Version (Child Explanation): Imagine a child who follows rules immediately. If they see a toy on the floor (the current percept), their only rule is "If toy is on floor, pick up." They don't remember if the toy was there yesterday or if they just put it down (they ignore past history). They just react based on the rule.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists The structure shows a direct mapping from percepts to action via Condition-action rules:
| Sensors → What the world is like now → Condition-action rules → What action I should do now → Actuators |
Output or Result This agent has limited intelligence, struggles with non-perceptual parts of the state, and is not adaptive to environmental changes.
--------------------------------------------------------------------------------
2.3.2 Model-based Reflex Agent
Explanation These agents can work in a partially observable environment because they track the situation. They rely on two factors: a Model (knowledge about "how things happen in the world") and an Internal State (a representation of the current state based on the percept history). They use this model and their updated internal state to decide on actions. Updating the state requires knowing how the world evolves and how the agent's actions affect the world.
Better Version (Child Explanation): This is like a child who keeps a mental picture of the world, even if they can’t see everything right now. If they see a ball roll behind a sofa (current percept), they know the ball is still there (internal state), even though they can't see it (partially observable environment). This is because they have a 'model' of the world—they know that balls don't disappear when they roll behind furniture.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists The structure incorporates the 'State' information, which is updated by knowing "How the world evolves" and "What my actions do".
Output or Result These agents can make better decisions than simple reflex agents because they have an internal representation (state) of the hidden parts of the world,.
--------------------------------------------------------------------------------
2.3.3 Goal-based agents
Explanation These agents expand on the model-based approach by incorporating a "goal," which describes desirable future situations. Current state knowledge is insufficient; the agent must know what to do to reach the goal. They choose actions that help achieve the goal, often requiring them to consider a long sequence of possible actions before deciding (a process called searching and planning), which makes them proactive.
Better Version (Child Explanation): This child knows where they want to go (the Goal). If they want a specific toy across the room, they don't just react to the floor (like the simple agent), and they don't just know where the toy is (like the model-based agent). They plan: "I need to walk past the table, step over the cat, and then grab the toy." They look ahead to make sure their steps help them reach the final destination.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists The structure shows the 'Goals' box influencing the "What action I should do now" decision.
Output or Result The agent exhibits proactive behaviour, planning sequences of actions to reach a specific desirable goal state,.
--------------------------------------------------------------------------------
2.3.4 Utility-based agents
Explanation These agents are similar to goal-based agents but add a utility measurement component, which provides a measure of success at a given state. They act based not only on achieving the goal but also on finding the best way to achieve it. The utility function maps each state to a real number to check how efficiently each action accomplishes the goal. These agents are essential when there are multiple alternative paths, and the agent must choose the one that provides the highest "happiness" or efficiency.
Better Version (Child Explanation): This is the smartest planner. If this child wants to get the specific toy (Goal), and there are two ways: one path is fast but dirty, and the other path is slow but clean. The child uses a Utility score—how much they value speed versus cleanliness—to pick the absolute best path. They choose the action that maximizes their happiness (utility).
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists The structure includes a 'Utility' box with the question, "How happy I will be In such a state," influencing the action selection.
Output or Result The agent selects the action sequence that maximizes the expected utility, ensuring the goal is achieved in the most efficient or desirable way possible.
--------------------------------------------------------------------------------
2.3.5 Learning Agents
Explanation A learning agent can learn from its past experiences and adapt automatically. It starts with basic knowledge and continually improves.
A learning agent has four conceptual components:
1. Learning element: Responsible for making improvements by learning from the environment.
2. Critic: Provides feedback to the learning element, describing how well the agent is performing compared to a fixed standard.
3. Performance element: Responsible for selecting the external action.
4. Problem generator: Suggests actions that will lead to new, informative experiences for the agent.
Better Version (Child Explanation): This is a child who practices and gets better. The Performance element tries to build the toy tower. The Critic watches and says, "That tower is too wobbly!" The Learning element uses this feedback to figure out how to build a stronger tower next time. The Problem generator then suggests trying a slightly different block arrangement so the child learns something new.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists The diagram shows the interaction between the four components, with feedback flowing from the Critic to the Learning element, and actions generated by the Performance element.
Output or Result Learning agents are capable of learning, analysing their performance, and searching for new ways to improve their actions over time.
--------------------------------------------------------------------------------
3. Rational Agent
Explanation A rational agent is defined by having clear preferences, modelling uncertainty, and acting in a way that maximises its performance measure based on all possible actions. A rational agent performs the "right things," and AI aims to create these agents for use in decision theory and game theory. Rational actions are crucial because, in reinforcement learning, the agent receives a positive reward for the best possible action and a negative reward for the wrong action.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists (No specific diagram is provided.)
Output or Result The action taken by a rational agent should be the one that is expected to maximise its defined performance measure.
--------------------------------------------------------------------------------
3.1 Rationality
Explanation The rationality of an agent is measured by its performance measure. Rationality is judged based on four key points:
1. The Performance measure (which defines the success criterion).
2. The agent’s prior knowledge of its environment.
3. The best possible actions the agent can perform.
4. The sequence of percepts (observations).
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists (No specific diagram is provided.)
Output or Result An action is rational if, based on the percept sequence and prior knowledge, it leads to the maximum expected value of the performance measure.
--------------------------------------------------------------------------------
4. Nature of Environment
Explanation Environments can be categorized by eight features from the agent’s perspective.
4.1 Fully observable vs Partially Observable Explanation: If an agent’s sensors can access the complete state of the environment at all times, it is fully observable. If not, it is partially observable. A fully observable environment is easier to deal with as the agent doesn't need to maintain an internal state history. Output or Result: Determines the need for an internal state representation within the agent.
4.2 Deterministic vs Stochastic Explanation: In a deterministic environment, the agent's current state and selected action entirely determine the next state. A stochastic environment is random and cannot be completely determined by the agent. If an environment is deterministic and fully observable, the agent does not need to worry about uncertainty. Output or Result: Determines whether the outcome of an action is predictable.
4.3 Episodic vs Sequential Explanation: In an episodic environment, actions are a series of one-shot events, and only the current percept is needed for the action decision. In a sequential environment, the agent requires memory of past actions to determine the next best action. Output or Result: Determines if the history of actions influences future decisions.
4.4 Single-agent vs Multi-agent Explanation: If only one agent operates in the environment, it is a single-agent environment. If multiple agents are operating, it is a multi-agent environment. Multi-agent environment design problems are different from single-agent ones. Output or Result: Defines the complexity of interaction and competition/cooperation needed for agent design.
4.5 Static vs Dynamic Explanation: A dynamic environment changes while the agent is deliberating (thinking). A static environment does not change during deliberation. Static environments are easier because the agent doesn't need to continuously monitor the world while deciding. Taxi driving is dynamic, while crossword puzzles are static. Output or Result: Determines if the agent needs to continuously monitor the world during decision making.
4.6 Discrete vs Continuous Explanation: If an environment has a finite number of percepts and actions, it is discrete (e.g., a chess game). If the percepts and actions flow smoothly and are infinite, it is continuous (e.g., a self-driving car). Output or Result: Defines whether the state space and action space are finite or infinite.
4.7 Known vs Unknown Explanation: This refers to the agent's state of knowledge. In a known environment, the agent knows the results of all actions. In an unknown environment, the agent must learn how it works. Output or Result: Defines whether the agent needs to incorporate a learning process.
4.8 Accessible vs Inaccessible Explanation: An environment is accessible if the agent can obtain complete and accurate information about the environment's state. Otherwise, it is inaccessible. An empty room whose state is defined by temperature is accessible; information about a world event is inaccessible. Output or Result: Defines the quality and completeness of the information the agent can gather.
--------------------------------------------------------------------------------
5. Structure of an Al Agent
Explanation The structure of an intelligent agent is a combination of its architecture and its agent program.
Agent=Architecture+Agent program
1. Architecture: The physical machinery or platform on which the AI agent executes.
2. Agent Function: A conceptual map that links a percept sequence to an action, represented as f:P 
∗
 →A.
3. Agent program: The concrete implementation of the agent function, which runs on the physical architecture.
5.1 PEAS Representation
Explanation PEAS is a model used to define the properties of an AI agent, where performance measure is the objective for the agent's success.
• P: Performance measure
• E: Environment
• A: Actuators
• S: Sensors
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists PEAS Example (Self-driving car):
Agent
Performance measure
Environment
Actuators
Sensors
Self-driving car
Safety, time, legal drive, comfort
Roads, other vehicles, road signs, pedestrian
Steering, accelerator, brake, signal, horn
Camera, GPS, speedometer, odometer, accelerometer, sonar
Output or Result PEAS provides a structured description of the elements required for a rational agent to operate successfully in a specific environment.
--------------------------------------------------------------------------------
6. Problem-solving agents
Explanation Problem-solving agents are a type of goal-based agent that use atomic representation. These agents rely on search strategies or algorithms to find the best result for a specific problem.
Steps to Solve a Problem Using AI:
1. Analyzing the Problem
2. Defining the Problem
3. Identification of Solutions
4. Choosing the Solution
5. Implementation
Measuring problem-solving performance:
1. Completeness: Does the algorithm guarantee finding a solution if one exists?
2. Optimality: Does the strategy find the best solution?
3. Time complexity: How long does it take to find a solution?
4. Space complexity: How much memory is needed for the search?
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A figure illustrating the five steps of problem solving in AI:
| ANALYZING THE PROBLEM → DEFINING THE PROBLEM → IDENTIFICATION OF SOLUTIONS → CHOOSING THE SOLUTION → IMPLEMENTATION |
Output or Result The output is an optimal solution found via a systematic search, measured by its time, space, completeness, and optimality,.
--------------------------------------------------------------------------------
7. Problem Searching
Explanation Searching is a step-by-step procedure used to solve a search problem within a given search space, and it is the most common technique for problem solving in AI,.
7.1 Search Algorithm Terminologies:
• Search Space: The set of all possible solutions a system may have.
• Start State: The state from which the agent begins the search.
• Goal test: A function that checks the current state and returns whether the goal state has been achieved.
• Search tree: A tree representation of the search problem, where the root node corresponds to the initial state.
• Path Cost: A function assigning a numeric cost to each path.
• Solution: An action sequence leading from the start node to the goal node.
• Optimal Solution: The solution that has the lowest cost among all solutions.
7.3 Types of search algorithms Search algorithms are classified into two main categories:
1. Uninformed Search (Blind search): Algorithms that do not use domain knowledge or information about the goal's location. They operate in a brute-force way, traversing the tree without external guidance.
2. Informed Search (Heuristic search): Algorithms that use domain knowledge and problem information to guide the search, allowing them to find solutions more efficiently. They often use a heuristic, which guarantees a good solution in a reasonable time, though not always the absolute best solution.
--------------------------------------------------------------------------------
8. Uninformed Search Algorithms
Explanation Uninformed search algorithms operate in a brute-force manner and lack additional information about the search space, earning them the name "blind search",.
8.1 Breadth-first Search (BFS)
Explanation BFS is a common strategy that searches breadth-wise (level by level) in a tree or graph,. It starts at the root node and expands all successor nodes at the current level before moving to the next level's nodes. BFS is implemented using a FIFO (First-In, First-Out) queue data structure.
Better Version (Child Explanation): Imagine you are looking for a hidden toy in a tall house. BFS means you check every room on the ground floor first. Once you have checked all rooms on the ground floor, you move up and check all rooms on the first floor. You keep checking level by level until you find the toy. This method guarantees you find the toy using the fewest possible steps.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A diagram showing traversal order: S → A, B, C, D (Level 1) → G, H, E, F (Level 2) → I, K (Level 3/4),.
Output or Result
• Completeness: BFS is complete; it will find a solution if one exists at a finite depth.
• Optimality: BFS is optimal if the path cost is a non-decreasing function of the node depth.
• Time Complexity: O(b 
d
 ), where d is the depth of the shallowest solution and b is the branching factor.
• Space Complexity: O(b 
d
 ), as it saves every level into memory,.
--------------------------------------------------------------------------------
8.2 Depth-first Search (DFS)
Explanation DFS is a recursive algorithm that starts at the root node and follows one path to its greatest depth node before backtracking and moving to the next path. DFS uses a stack data structure for implementation.
Better Version (Child Explanation): Using the same house, DFS means you enter a room (Start) and immediately follow one path as far as it goes—maybe you go through a door into a closet, then a trunk inside the closet, then a box inside the trunk. Only once you hit a dead end (the maximum depth), do you come back one step (backtrack) and try the next path.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A diagram showing traversal order: S → A → B → D → E (backtrack) → C → G (Goal),.
Output or Result
• Completeness: DFS is complete within a finite state space.
• Optimality: DFS is non-optimal, as it may find a high-cost path before finding the best solution.
• Time Complexity: O(n 
m
 ), where m is the maximum depth of any node.
• Space Complexity: O(b 
m
 ), as it only needs to store a single path (stack) from root to current node,.
--------------------------------------------------------------------------------
8.3 Depth-Limited Search Algorithm (DLS)
Explanation DLS is essentially DFS but with a predetermined depth limit (l). Nodes at this depth limit are treated as if they have no successors, preventing the algorithm from running into infinite loops (a common DFS problem).
Failure Conditions: DLS can terminate with a Standard failure value (no solution exists) or a Cutoff failure value (no solution exists within the given depth limit),.
Better Version (Child Explanation): This is like the deep-diving child (DFS), but before they start, a parent tells them: "You can only search three rooms deep, no further." If the toy is found within those three rooms, great. If the toy is in the fourth room, the search stops and says, "Cutoff failure!" because it hit the boundary.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A tree structure illustrating the levels/depth.
Output or Result
• Completeness: DLS is complete if the solution is above the depth limit.
• Optimality: DLS is not optimal, even if the limit is deeper than the solution.
• Time Complexity: O(b 
l
 ).
• Space Complexity: O(b×l).
--------------------------------------------------------------------------------
8.4 Uniform-cost Search Algorithm (UCS)
Explanation UCS is used for traversing a weighted tree or graph, focusing on finding a path to the goal node that has the lowest cumulative cost. It expands nodes based on their path costs from the root node, giving maximum priority to the lowest cumulative cost path,. UCS is implemented using a priority queue. If all edge costs are the same, UCS is equivalent to BFS.
Better Version (Child Explanation): If the house has costs associated with moving—like climbing stairs costs 5 points, but walking through a dusty hallway costs 1 point—UCS always chooses the path that has the cheapest total score so far. It doesn't care if the cheap path takes 10 steps or 2 steps; it only cares about the total cost (the price tag), meaning it might get stuck cycling between two low-cost rooms forever if the costs are zero (infinite loop risk).
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A tree showing varying edge costs (weights).
Output or Result
• Completeness: UCS is complete if a solution exists.
• Optimality: UCS is always optimal because it chooses the path with the lowest cost at every state,.
• Time Complexity: O(b 
1+[C 
∗
 /ϵ]
 ), where C 
∗
  is the optimal solution cost.
• Space Complexity: O(b 
1+[C 
∗
 /ϵ]
 ).
--------------------------------------------------------------------------------
8.5 Iterative deepening depth-first Search (IDDFS)
Explanation IDDFS combines the strengths of DFS and BFS: the memory efficiency of DFS and the completeness/optimality advantages of BFS. It gradually increases the depth limit (l) in successive iterations, performing a DLS up to that limit until the goal is found. It is useful when the search space is large and the depth of the goal node is unknown. The main drawback is that it repeats all the work of previous phases.
Better Version (Child Explanation): This child uses the DLS strategy, but they are smart about it. They search only 1 room deep. If they fail, they immediately start over and search 2 rooms deep. If they fail, they start over and search 3 rooms deep, and so on. They keep increasing the depth limit until they find the toy. They get the speed of searching quickly (DFS style) while guaranteeing they find the shortest path (BFS style).
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A tree structure illustrating the increasing iterations (e.g., 1st Iteration: A; 4th Iteration: A, B, D, H, I, E, C, F, K, G),.
Output or Result
• Completeness: Complete if the branching factor is finite.
• Optimality: Optimal if path cost is a non-decreasing function of the depth of the node.
• Time Complexity: O(b 
d
 ).
• Space Complexity: O(bd).
--------------------------------------------------------------------------------
8.6 Bidirectional Search Algorithm
Explanation Bidirectional search runs two simultaneous searches: one forward-search from the initial state and one backward-search from the goal state. It replaces one large search graph with two smaller subgraphs. The search terminates when the two graphs intersect each other. It can utilize techniques like BFS, DFS, or DLS. This method is fast and requires less memory.
Better Version (Child Explanation): Instead of the child starting at the front door and looking for the toy, the child starts at the front door and simultaneously, a parent starts at the toy’s location (the Goal) and starts looking backward toward the child. They both walk towards the middle, and the search ends when they meet. This makes finding the target much faster because they cover the total distance from both ends.
Codes if exists (No programming code examples are provided in the sources.)
Diagrams if exists A diagram showing Node 1 (Root node) and Node 16 (Goal node) searching towards a meeting point (Intersection Node 9).
Output or Result
• Completeness: Complete if BFS is used for both searches.
• Optimality: Bidirectional search is Optimal.
• Time Complexity: O(b 
d
 ).
• Space Complexity: O(b 
d
 ).
